---
title: "Question 4.2"
author: "Siddharth Gandhi"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r 4.2 With Scaling}

#This code will be with scaling

#Load library
library(factoextra)

#Load data
original_data <- read.table("C:\\Users\\Sid\\OneDrive\\Desktop\\Data 4.2\\iris.txt", 
                   stringsAsFactors = F, header = T)

#Scaling the data and getting rid of the categorical variables
data <- scale(original_data[,1:4])

set.seed(1)

#Generate an elbow plot to see which number of clusters might be the best
fviz_nbclust(data, kmeans, method="wss")

#Based on the results of the elbow plot, k=3 clusters seems best
#But generate a few more to see how they would do 
cluster2 <- kmeans(as.matrix(data), 2, nstart=10)
cluster2
table(cluster2$cluster, original_data$Species)

cluster3 <- kmeans(as.matrix(data), 3, nstart=10)
cluster3
table(cluster3$cluster, original_data$Species)

cluster4 <- kmeans(as.matrix(data), 4, nstart=10)
cluster4
table(cluster4$cluster, original_data$Species)

#For each cluster, look at the quality of the partition
#This ratio indicates the amount of information retained after clustering
quality <- rep(0, 3)

for (i in 1:4) {
  k_model <- kmeans(as.matrix(data), i, nstart=10)
  quality[i] <- (k_model$betweenss / k_model$totss) * 100
  
}


quality_matrix <- as.matrix(quality[2:4])
rownames(quality_matrix) <- c("k2", "k3", "k4")
colnames(quality_matrix) <- "Percent quality of partition"
quality_matrix

#Generate a function to find the best combination of predictors
#The accuracy with all four predictors

accuracy_four <- (50 + 39 + 36)/150
accuracy_four

#Using only three inputs because we already got the prediction with four above
accuracy <- function(p1, p2, p3){
  if (nargs() == 3) {
    input <- c(p1, p2, p3)
  }
  if (nargs() == 2) {
    input <- c(p1, p2)
  }
  if (nargs() == 1) {
    input <- p1
  }
  model <- kmeans(data[,input], 3, nstart=10)
  table(model$cluster, original_data$Species)
}

#Calculate accuracy
accuracy(1,2,3)
acc1 <- (35 + 37 + 49)/150

accuracy(1,2,4)
acc2 <- (34 + 49 + 38)/150

accuracy(1,3,4)
acc3 <- (44 + 50 + 36)/150

accuracy(2,3,4)
acc4 <- (49 + 38 + 42)/150

accuracy(1,2)
acc5<- (49 + 36 + 31)/150
  
accuracy(1,3)
acc6<- (50 + 37 + 34)/150
  
accuracy(1,4)
acc7<- (50 + 40 + 35)/150
  
accuracy(2,3)
acc8<- (49 + 27 + 40)/150
  
accuracy(2,4)
acc9<- (49 + 39 + 35)/150
  
accuracy(3,4)
acc10<- (50 + 48 + 46)/150
  
accuracy(1)
acc11<- (47 + 22 + 31)/150
  
accuracy(2)
acc12<- (31 + 21 + 34)/150
  
accuracy(3)
acc13<- (48 + 44 + 50)/150
  
accuracy(4)
acc14<-  (48 + 46 + 50)/150

acc_vec <- c(acc1, acc2, acc3, acc4, acc5, acc6, acc7, acc8, acc9, acc10, acc11,
          acc12, acc13, acc14)
max(acc_vec)
which.max(acc_vec)

#Using predictors 3 and 4 (petal length and width) gives the best results

```

~Split~

```{r 4.2 Without Scaling}

#This code will be without scaling

#Load library
library(factoextra)

#Load data
original_data <- read.table("C:\\Users\\Sid\\OneDrive\\Desktop\\Data 4.2\\iris.txt", 
                   stringsAsFactors = F, header = T)

#Scaling the data and getting rid of the categorical variables
data <- original_data[,1:4]

set.seed(2)

#Generate an elbow plot to see which number of clusters might be the best
fviz_nbclust(data, kmeans, method="wss")

#Based on the results of the elbow plot, k=3 clusters seems best
#But generate a few more to see how they would do 
cluster2 <- kmeans(as.matrix(data), 2, nstart=10)
cluster2
table(cluster2$cluster, original_data$Species)

cluster3 <- kmeans(as.matrix(data), 3, nstart=10)
cluster3
table(cluster3$cluster, original_data$Species)

cluster4 <- kmeans(as.matrix(data), 4, nstart=10)
cluster4
table(cluster4$cluster, original_data$Species)

#For each cluster, look at the quality of the partition
#This ratio indicates the amount of information retained after clustering
quality <- rep(0, 3)

for (i in 1:4) {
  k_model <- kmeans(as.matrix(data), i, nstart=10)
  quality[i] <- (k_model$betweenss / k_model$totss) * 100
  
}


quality_matrix <- as.matrix(quality[2:4])
rownames(quality_matrix) <- c("k2", "k3", "k4")
colnames(quality_matrix) <- "Percent quality of partition"
quality_matrix

#Generate a function to find the best combination of predictors
#The accuracy with all four predictors

accuracy_four <- (50 + 48 + 36)/150
accuracy_four

#Using only three inputs because we already got the prediction with four above
accuracy <- function(p1, p2, p3){
  if (nargs() == 3) {
    input <- c(p1, p2, p3)
  }
  if (nargs() == 2) {
    input <- c(p1, p2)
  }
  if (nargs() == 1) {
    input <- p1
  }
  model <- kmeans(data[,input], 3, nstart=10)
  table(model$cluster, original_data$Species)
}

#Calculate accuracy
accuracy(1,2,3)
acc1 <- (50 + 45 + 37)/150

accuracy(1,2,4)
acc2 <- (50 + 39 + 35)/150

accuracy(1,3,4)
acc3 <- (50 + 48 + 36)/150

accuracy(2,3,4)
acc4 <- (50 + 48 + 45)/150

accuracy(1,2)
acc5<- (50 + 38 + 35)/150
  
accuracy(1,3)
acc6<- (50 + 45 + 37)/150
  
accuracy(1,4)
acc7<- (50 + 40 + 35)/150
  
accuracy(2,3)
acc8<- (50 + 37 + 37)/150
  
accuracy(2,4)
acc9<- (49 + 46 + 44)/150
  
accuracy(3,4)
acc10<- (50 + 48 + 46)/150
  
accuracy(1)
acc11<- (47 + 22 + 31)/150
  
accuracy(2)
acc12<- (31 + 21 + 34)/150
  
accuracy(3)
acc13<- (48 + 44 + 50)/150
  
accuracy(4)
acc14<-  (48 + 46 + 50)/150

acc_vec <- c(acc1, acc2, acc3, acc4, acc5, acc6, acc7, acc8, acc9, acc10, acc11,
          acc12, acc13, acc14)
max(acc_vec)
which.max(acc_vec)

#Using predictors 3 and 4 (petal length and width) gives the best results

```
